{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": " Celebrity Classifier with VGG16.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae8YyJCRMDaR",
        "colab_type": "text"
      },
      "source": [
        "# Making a Celebrity Classifier with VGG16\n",
        "\n",
        "### Loading the VGG16 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eehaGvTrMDaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2af3d465-fdbc-47a1-f52b-04d9d5bdfd75"
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
        "img_rows = 224\n",
        "img_cols = 224 \n",
        "\n",
        "#Loads the VGG16 model \n",
        "model = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IE7e-nRMDaZ",
        "colab_type": "text"
      },
      "source": [
        "### Inpsecting each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTB6R1BXMDaa",
        "colab_type": "code",
        "outputId": "3ae79bd7-9654-45ea-b0e4-3b18ed9b304d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(model.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D True\n",
            "2 Conv2D True\n",
            "3 MaxPooling2D True\n",
            "4 Conv2D True\n",
            "5 Conv2D True\n",
            "6 MaxPooling2D True\n",
            "7 Conv2D True\n",
            "8 Conv2D True\n",
            "9 Conv2D True\n",
            "10 MaxPooling2D True\n",
            "11 Conv2D True\n",
            "12 Conv2D True\n",
            "13 Conv2D True\n",
            "14 MaxPooling2D True\n",
            "15 Conv2D True\n",
            "16 Conv2D True\n",
            "17 Conv2D True\n",
            "18 MaxPooling2D True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwpfFz0pMDae",
        "colab_type": "text"
      },
      "source": [
        "### Let's freeze all layers except the top 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOhMckMBMDaf",
        "colab_type": "code",
        "outputId": "fac39bda-af5c-4521-9cb4-61428edcc9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
        "img_rows = 224\n",
        "img_cols = 224 \n",
        "\n",
        "# Re-loads the VGG16 model without the top or FC layers\n",
        "model = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))\n",
        "\n",
        "# Here we freeze the last 4 layers \n",
        "# Layers are set to trainable as True by default\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(model.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4ATHBe4MDaj",
        "colab_type": "text"
      },
      "source": [
        "### Let's make a function that returns our FC Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JudntFuVMDak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addTopModel(bottom_model, num_classes, D=256):\n",
        "    \"\"\"creates the top or head of the model that will be \n",
        "    placed ontop of the bottom layers\"\"\"\n",
        "    top_model = bottom_model.output\n",
        "    top_model = Flatten(name = \"flatten\")(top_model)\n",
        "    top_model = Dense(D, activation = \"relu\")(top_model)\n",
        "    top_model = Dense(D, activation = \"relu\")(top_model)\n",
        "    top_model = Dropout(0.3)(top_model)\n",
        "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
        "    return top_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICJC5v69MDap",
        "colab_type": "code",
        "outputId": "423ddae2-e89d-4323-f501-d3be0bfbfc2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.input"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'input_2:0' shape=(None, 224, 224, 3) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jFoAVdJMDat",
        "colab_type": "code",
        "outputId": "f573886e-c8ba-4060-85a8-8fda5af460ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "model.layers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7f79803566a0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f79803566d8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f7980356940>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f7980356e80>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f7980356cf8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f7980360f28>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f79ca4de320>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f798035b630>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f798035bd68>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f7980362c18>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f79803676a0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f79803674e0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f7980367fd0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f798036fd68>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f79803767f0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f7980376630>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f79800f7320>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f79800f7eb8>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f79800fe940>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1RLpyn8MDax",
        "colab_type": "text"
      },
      "source": [
        "### Let's add our FC Head back onto VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRRCZGp6MDay",
        "colab_type": "code",
        "outputId": "cf65a546-1da3-4d38-9ab3-1c320ab14769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "num_classes = 3\n",
        "\n",
        "FC_Head = addTopModel(model, num_classes)\n",
        "\n",
        "modelnew = Model(inputs=model.input, outputs=FC_Head)\n",
        "\n",
        "print(modelnew.summary())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 21,204,035\n",
            "Trainable params: 6,489,347\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMO18nSxMDa6",
        "colab_type": "text"
      },
      "source": [
        "### Loading our Celebrity Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXjpBkIJMDa6",
        "colab_type": "code",
        "outputId": "ea94e57a-d84b-46d6-cc7e-b4c4e8f7373a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_dir = '/content/drive/My Drive/dataset/train/'\n",
        "validation_data_dir = '/content/drive/My Drive/dataset/test/'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "# Change the batchsize according to your system RAM\n",
        "train_batchsize = 16\n",
        "val_batchsize = 10\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=val_batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 375 images belonging to 3 classes.\n",
            "Found 101 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8yYA83iMDa-",
        "colab_type": "text"
      },
      "source": [
        "### Training our top layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dq6H3avW8Hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sie1hQITMDa_",
        "colab_type": "code",
        "outputId": "9e2607d2-28f1-4e2c-be24-6d5f0fde45bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "                   \n",
        "checkpoint = ModelCheckpoint(\"celebrity_vgg.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 3,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "callbacks = [earlystop, checkpoint]\n",
        "\n",
        "# Note we use a very small learning rate \n",
        "modelnew.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = RMSprop(lr = 0.001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "nb_train_samples = 375\n",
        "nb_validation_samples = 101\n",
        "epochs = 6\n",
        "batch_size = 16\n",
        "\n",
        "history = modelnew.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size)\n",
        "\n",
        "modelnew.save(\"celebrity_vgg.h5\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 206 bytes but only got 0. Skipping tag 0\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 724 bytes but only got 0. Skipping tag 0\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 65536 bytes but only got 0. Skipping tag 0\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 170 bytes but only got 0. Skipping tag 0\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 178 bytes but only got 0. Skipping tag 0\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 9. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 180s 8s/step - loss: 4.1087 - accuracy: 0.4234 - val_loss: 0.4093 - val_accuracy: 0.6167\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.40933, saving model to celebrity_vgg.h5\n",
            "Epoch 2/6\n",
            "23/23 [==============================] - 14s 588ms/step - loss: 1.1737 - accuracy: 0.5125 - val_loss: 0.6208 - val_accuracy: 0.8235\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.40933\n",
            "Epoch 3/6\n",
            "23/23 [==============================] - 10s 417ms/step - loss: 0.9558 - accuracy: 0.5599 - val_loss: 0.3070 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.40933 to 0.30700, saving model to celebrity_vgg.h5\n",
            "Epoch 4/6\n",
            "23/23 [==============================] - 8s 355ms/step - loss: 0.6931 - accuracy: 0.7019 - val_loss: 0.0145 - val_accuracy: 0.4706\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.30700 to 0.01447, saving model to celebrity_vgg.h5\n",
            "Epoch 5/6\n",
            "23/23 [==============================] - 9s 406ms/step - loss: 0.8310 - accuracy: 0.6848 - val_loss: 1.1454 - val_accuracy: 0.7000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01447\n",
            "Epoch 6/6\n",
            "23/23 [==============================] - 9s 371ms/step - loss: 0.6110 - accuracy: 0.7571 - val_loss: 0.1017 - val_accuracy: 0.6275\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XUDJrJzWgR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "celebrity_dict = {\"[0]\": \"Amitab \", \n",
        "                      \"[1]\": \"deepika\",\n",
        "                      \"[2]\": \"varun\"\n",
        "                            }\n",
        "\n",
        "celebrity_dict_n = {\"amitab\": \"Amitab \", \n",
        "                      \"deepika\": \"deepika\",\n",
        "                      \"varun\": \"varun\",\n",
        "                      }\n",
        "\n",
        "def draw_test(name, pred, im):\n",
        "    monkey = celebrity_dict [str(pred)]\n",
        "    BLACK = [0,0,0]\n",
        "    expanded_image = cv2.copyMakeBorder(im, 80, 0, 0, 100 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
        "    cv2.putText(expanded_image, monkey, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
        "    cv2.imshow(name, expanded_image)\n",
        "\n",
        "def getRandomImage(path):\n",
        "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
        "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
        "    random_directory = np.random.randint(0,len(folders))\n",
        "    path_class = folders[random_directory]\n",
        "    print(\"Class - \" + celebrity_dict_n[str(path_class)])\n",
        "    file_path = path + path_class\n",
        "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
        "    random_file_index = np.random.randint(0,len(file_names))\n",
        "    image_name = file_names[random_file_index]\n",
        "    return cv2.imread(file_path+\"/\"+image_name)    \n",
        "\n",
        "for i in range(0,10):\n",
        "    input_im = getRandomImage(\"dataset/test/\")\n",
        "    input_original = input_im.copy()\n",
        "    input_original = cv2.resize(input_original, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
        "    \n",
        "    input_im = cv2.resize(input_im, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
        "    input_im = input_im / 255.\n",
        "    input_im = input_im.reshape(1,224,224,3) \n",
        "    \n",
        "    # Get Prediction\n",
        "    res = np.argmax(classifier.predict(input_im, 1, verbose = 0), axis=1)\n",
        "    \n",
        "    # Show image with predicted class\n",
        "    draw_test(\"Prediction\", res, input_original) \n",
        "    \n",
        "    cv2.waitKey(0)\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}